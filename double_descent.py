# -*- coding: utf-8 -*-
"""Double Descent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JFzH1PSbneDzAwIjmuqvHuCtq_yml0Yg

## Introduction to Double Descent

Double descent is a curious and exciting concept in modern machine learning. Traditionally, we believed that making a model more complex would initially improve its performance, but after a point, it would start to get worse—this is the classic bias-variance trade-off.

However, with today's complex models like deep neural networks, something surprising happens. After a certain point, if we keep increasing the complexity, the model's performance actually improves again! This unexpected behavior is called **double descent**.

Understanding double descent helps us build better machine learning models. It shows that sometimes, making models bigger and more complex can actually lead to better results, even when it seems counterintuitive.

# 1.Model Complexity and Dataset size:
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

"""## Generate random data for demonstration"""

np.random.seed(42)
num_samples = 200
num_features = 2000
X = np.random.rand(num_samples, num_features)
y = np.random.rand(num_samples)

"""## Split the data into training and test sets"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""## Create and train the linear regression model for different model complexities"""

complexities = [1,10,20,30,40,100,1000,2000]  # Different model complexities
train_loss = []
test_loss = []
for complexity in complexities:
    # linear regression model
    model = LinearRegression()
    model.fit(X_train[:, :complexity], y_train)

    # Predict on the training set and calculate the training loss
    y_train_pred = model.predict(X_train[:, :complexity])
    train_loss.append(mean_squared_error(y_train, y_train_pred))

    # Predict on the test set and calculate the test loss
    y_test_pred = model.predict(X_test[:, :complexity])
    test_loss.append(mean_squared_error(y_test, y_test_pred))

"""##  Plot the train loss and test loss against model complexity"""

plt.plot(complexities, train_loss, marker='o', label='Train Loss')
plt.plot(complexities, test_loss, marker='o', label='Test Loss')
plt.xlabel('Model Complexity')
plt.ylabel('Mean Squared Error')
plt.title('Train Loss and Test Loss vs. Model Complexity')
plt.legend()
plt.show()

"""###In the previous scenario,In Unregularized Linear Regression: the model can perfectly memorize the training data, resulting in `zero` training error. However, when applied to test data, the model tends to perform poorly due to overfitting. The model becomes too complex and captures noise rather than the underlying patterns, leading to a high test error. We observe as the complexity exceeds the number of features `>200` the capacity of the model is increased and the test error becomes to decrease again after hitting the peak at interpolation point where number of features is equal to model complexity (number of parameters).

# 2.Regularization:

### Regularization helps control the complexity of the model and prevent overfitting. regularization plays a significant role in the occurrence of double descent. By experimenting with different regularization techniques and varying the strength of regularization

### Ridge Regression
"""

# Set up the parameters
np.random.seed(42)
num_feat = 600
n_repeats = 5
train_n_values = np.arange(10,  2*num_feat + 1, 10)  # Different values of training set size to use
alpha_values = [0, 0.1, 0.01, 1, 100, 1000,10000]  # Different values of regularization parameter to use
test_errors = np.zeros((len(alpha_values), len(train_n_values), n_repeats))

def generate_data(n, num_feat):
    return np.random.normal(0, 1, size=(n, num_feat))

def generate_labels(X, a_true):
    return np.sign(X.dot(a_true))

def compute_regression(X, y, alpha):
    A = np.dot(X.T, X) + alpha * np.eye(num_feat)
    B = np.dot(X.T, y)
    return np.linalg.solve(A, B)

def compute_test_error(X_test, y_test, a):
    y_pred = np.sign(X_test.dot(a))
    return np.mean(y_pred != y_test)

for i, alpha in enumerate(alpha_values):
    for j, train_n in enumerate(train_n_values):
        for k in range(n_repeats):
            X_train = generate_data(train_n, num_feat)
            a_true = generate_data(num_feat, 1)
            y_train = generate_labels(X_train, a_true)

            a = compute_regression(X_train, y_train, alpha)

            test_n = 1000
            X_test = generate_data(test_n, num_feat)
            y_test = generate_labels(X_test, a_true)

            test_errors[i, j, k] = compute_test_error(X_test, y_test, a)

mean_test_errors = np.mean(test_errors, axis=2)
std_test_errors = np.std(test_errors, axis=2)

# Iterate over each alpha value and plot separately
for i, alpha in enumerate(alpha_values):
    # Create a new figure and axis for each alpha value
    fig, ax = plt.subplots()

    # Plot the test errors with error bars for the current alpha value
    ax.errorbar(train_n_values, mean_test_errors[i], yerr=std_test_errors[i], label=r'$\alpha$ = {}'.format(alpha))

    # Set the x-axis and y-axis labels
    ax.set_xlabel('Training set size')
    ax.set_ylabel('Test error')

    # Set the title of the plot
    ax.set_title('Regularization(alpha = {})'.format(alpha))

    # Add a grid
    ax.grid(True)

    # Add legend
    ax.legend()

    # Show the plot for the current alpha value
    plt.show()

"""###  during the double descent curve, smaller α values can cause significant test loss increases near the interpolation threshold due to overfitting but can benefit from the second descent phase with lower test loss in highly complex models. Larger α values lead to more stable test loss behavior,"""